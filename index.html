<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Joanna Sliwa</title>
    <link rel="stylesheet" href="style.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" referrerpolicy="no-referrer" />
</head>
<body>
    <div class="container">
        <div align="center">
            <h1>Joanna Sliwa</h1>
            <img src="./files/image.jpeg" alt="profile" class="avatar">
            <p>ELLIS & IMPRS-IS PhD Student at University Tübingen</p>

            <a href="../files/CV.pdf" download><b>CV</b></a> <br/>

            <ul class="icons">
                <li><a href="mailto:joanna.sliwa@uni-tuebingen.de"><i class="fas fa-envelope"></i></a></li>
                <li><a href="https://github.com/JoannaSliwa"><i class="fab fa-github"></i></a></li>
                <li><a href="https://www.linkedin.com/in/j-sliwa/"><i class="fab fa-linkedin"></i></a></li>
                <li><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/methoden-des-maschinellen-lernens/personen/"><i class="fas fa-university"></i></a></li>
                <li><a href="https://scholar.google.com/citations?user=88s4T-8AAAAJ&hl=pl"><i class="ai ai-google-scholar"></i></a></li>
                <li><a href="https://bsky.app/profile/joannasliwa.bsky.social"><img src="files/bluesky.png" class="bluesky-icon"></a></li>
            </ul>
        </div>
        <br/>
        <p>I work at the intersection of deep learning and probabilistic machine learning. I am studying the use of curvature-based Bayesian uncertainty to functionally enhance the training of neural networks. I'm based at the group of Philipp Hennig.</p>
        <p><strong>KEYWORDS:</strong></p>
        <div class="keywords">
            <a href="https://www.youtube.com/playlist?list=PL05umP7R6ij2YE8rRJSb-olDNbntAQ_Bx" class="keyword-box">Bayesian Deep Learning</a>
            <a href="https://arxiv.org/abs/2302.00487" class="keyword-box">Continual Learning</a>
            <a href="https://en.wikipedia.org/wiki/Kalman_filter" class="keyword-box">Kalman Filtering</a>
            <a href="https://en.wikipedia.org/wiki/Hessian_matrix" class="keyword-box">Second-order optimization</a>
            <a href="https://arxiv.org/abs/2106.09685" class="keyword-box">Low-rank Adaptation</a>
        
        </div>

        <p><strong>RESEARCH:</strong></p>
        <p>The currently prevailing practice is to evaluate network performance solely by the attained final validation loss/accuracy. By acknowledging the probabilistic nature of the problem, constructing and leveraging uncertainty, I aim to generalise training to realistic and practically relevant settings like continual learning. This requires algorithmic advances to enhance efficiency through matrix-free, compressed, iterative linear algebra, but also the establishment of new design patterns in deep learning, like Kalman filtering on the weight-space.</p>
        <p>Before my PhD, I was working in the field of causality, mainly exploring how to solve causal representation learning where the sources generate observations via a nonlinear transformation. During my education, I was studying various topics in computational neuroscience such as Brain Computer Interfaces, constructing 3D brain atlas and neuron simulation.</p>

        <p><strong>PREPRINTS:</strong></p>
        <ul class="no-bullet publication-list">
            <li>
                <img src="files/cl.png" alt="Publication Image" class="publication-image">
                <div class="publication-content">
                    <strong>Efficient Weight-Space Laplace-Gaussian Filtering and Smoothing for Sequential Deep Learning</strong><br/>
                    J Sliwa, F Schneider, N Bosch, A Kristiadi, P Hennig<br/>
                    2024 
                    • <a href="https://arxiv.org/abs/2410.06800"><i class="ai ai-arxiv"></i> arXiv</a> <br/>
                    <!--• <a href="https://github.com/a90952852/lr-lgf"><i class="fab fa-github"></i> GitHub</a><br/>-->
                    
                    <!-- Toggle for TLDR -->
                    <label class="tldr-toggle" for="tldr-checkbox1"> Easy-to-Understand Summary</label>
                    <input type="checkbox" id="tldr-checkbox1" class="tldr-checkbox">
                    <div class="tldr-content">
                       Imagine teaching a computer to spot tumor cells. As it learns from different hospitals, each with unique imaging styles, the computer forgets what it previously learned (e.g. in hospital 1, while learning images from hospital 5). Our method helps the model to remember its past knowledge while still learning new things efficiently.
<!-- Teaching a neural network multiple tasks, like recognizing images from different hospitals, one after another is challenging because the model tends to forget what it learned earlier when focusing on new tasks. Our method uses a Bayesian approach to help the model remember past knowledge while adapting to new tasks by understanding how they are related.-->
                    </div>
                </div>
            </li>
        </ul>

        <p><strong>PUBLICATIONS:</strong></p>
        <ul class="no-bullet publication-list">
            <li>
                <img src="files/lora-trade-off.png" alt="Publication Image" class="publication-image2" width="130" />

                <div class="publication-content">
                    <strong>Mitigating Forgetting in Low Rank Adaptation</strong><br/>
                    J Sliwa, F Schneider, P Hennig, JM Hernández-Lobato<br/>
                    Second Workshop on Test-Time Adaptation: Putting Updates to the Test! at ICML<br/>
                    2025
                    • <a href="https://openreview.net/forum?id=W3caXQAjcu">
                        <img src="files/circle.png" alt="OpenReview" class="openreview-icon" width="22" height="22" />
                        OpenReview
                      </a><br/>
                    
                    <!-- Toggle for TLDR -->
                    <label class="tldr-toggle" for="tldr-checkbox2"> Easy-to-Understand Summary</label>
                    <input type="checkbox" id="tldr-checkbox2" class="tldr-checkbox">
                    <div class="tldr-content">
                        Imagine specializing a language model for a new skill - say, learning another language or solving math problems. In the process it can forget earlier knowledge like grammar and common sense. We protect the knowledge the model is confident about and keep other parts flexible so it can still learn.
                </div>
            </li>
            <li>
                <img src="files/cocktail_party.png" alt="Publication Image" class="publication-image">
                <div class="publication-content">
                    <strong>Probing the Robustness of Independent Mechanism Analysis for Representation Learning</strong><br/>
                    J Sliwa, S Ghosh, V Stimper, L Gresele, B Schölkopf<br/>
                    First Causal Representation Learning Workshop at UAI<br/>
                    2022
                    • <a href="https://arxiv.org/abs/2207.06137"><i class="ai ai-arxiv"></i> arXiv</a> 
                    • <a href="https://github.com/lgresele/independent-mechanism-analysis"><i class="fab fa-github"></i> GitHub</a><br/>
                    
                    <!-- Toggle for TLDR -->
                    <label class="tldr-toggle" for="tldr-checkbox3"> Easy-to-Understand Summary</label>
                    <input type="checkbox" id="tldr-checkbox3" class="tldr-checkbox">
                    <div class="tldr-content">

                    Imagine teaching a computer to separate single voices from recordings of multiple people speaking in a crowded room. The method IMA, inspired by the concepts from the field of causality, has shown good results. Our work checks if the method still works when the conditions aren't perfectly met.
                        <!--The IMA method helps identify underlying factors from observations, e.g., separating individual voices from a recording of several people speaking at once. We tested whether the method still works when the conditions it relies on aren’t perfectly met.-->
                    </div>
                </div>
            </li>
        </ul>


        

        <p><strong>NEWS:</strong></p>
        <ul class="news-list">
            <li>
                <span class="news-date">07/2025</span>
                <span class="news-content">attended and presented at <a href="https://tta-icml2025.github.io/">PUT workshop</a> at ICML in Vancouver</span>
            </li>
            <li>
                <span class="news-date">06/2024</span>
                <span class="news-content">started an ELLIS exchange at <a href="https://cbl.eng.cam.ac.uk">Computational and Biological Learning Lab</a> at University of Cambridge with Miguel Lobato</span>
            </li>
            <li>
                <span class="news-date">04/2024</span>
                <span class="news-content">attended a <a href="https://probnumschool.org/pages/faq.html">Probabilistic Numerics Spring School in 
Southampton</a></span>
            </li>
            <li>
                <span class="news-date">09/2023</span>
                <span class="news-content">attended an <a href="https://fcai.fi/eds2023/home">ELLIS Doctoral Symposium in Helsinki</a> and gave a tooling session about JAX</span>
            </li>
            <li>
                <span class="news-date">06/2023</span>
                <span class="news-content">started my PhD at <a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/methoden-des-maschinellen-lernens/start/">Methods of Machine Learning Group</a> at University of Tübingen with Philipp Hennig</span>
            </li>
        </ul>
        Updated: July 2025
        <h1></h1>
    </div>
</html>
